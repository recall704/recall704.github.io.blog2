<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>K8s on RECALL&#39;s Blog</title>
    <link>http://www.recall704.com/tags/k8s/</link>
    <description>Recent content in K8s on RECALL&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 14 Aug 2018 23:06:42 +0800</lastBuildDate>
    
	<atom:link href="http://www.recall704.com/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>k8s pod 调度</title>
      <link>http://www.recall704.com/cloud/k8s-scheduler/</link>
      <pubDate>Tue, 14 Aug 2018 23:06:42 +0800</pubDate>
      
      <guid>http://www.recall704.com/cloud/k8s-scheduler/</guid>
      <description>一、k8s 基础架构 在分析 k8s pod 调度之前，我们先来回顾一下 k8s 的架构
二、k8s pod 调度相关的参数 当用户通过 kubectl 或者 api 创建了一个 deployment 或者 其它 workload 的时候， k8s 会创建对应的 pod 信息，pod 信息最终会传递给 kubelet，实例化成具体的 docker 容器， 那么 k8s 是根据什么条件，怎么把 pod “安排”到 node 上去的呢？ 有哪些因数会影响 pod 的调度呢？
我找了一下 pod 的参数，大概会有以下几个参数影响 pod 的调度
1. NodeName 2. NodeSelector 3. Affinity 4. Tolerations 5. PriorityClassName 6. Priority 1. nodeName 一旦你在 pod 里指定了 nodeName ,那么 pod 一定会调度到该节点上, 其实 pod 已经跳过了调度过程了
2. Priority pod 是可以有优先级的，这个优先级是相对于其它 pod 的，也就是说，如果 pod 的优先级高于其它 pod，这个 pod 会被优先调度。</description>
    </item>
    
    <item>
      <title>k8s nodeport 端口范围</title>
      <link>http://www.recall704.com/cloud/k8s-nodeport-range/</link>
      <pubDate>Tue, 14 Aug 2018 22:55:42 +0800</pubDate>
      
      <guid>http://www.recall704.com/cloud/k8s-nodeport-range/</guid>
      <description>k8s 的 nodeport 默认范围是 30000-32767，那这个端口范围到底是半开半闭区间，还是开区间，还是闭区间呢？ 而且能不能是不连续的呢？ 能不能像（80，443， 30000-32767）这样呢？
在 k8s 源码中搜索 32767，在
gopath/src/k8s.io/kubernetes/cmd/kube-apiserver/app/options/options.go  存在代码
fs.Var(&amp;amp;s.ServiceNodePortRange, &amp;quot;service-node-port-range&amp;quot;, &amp;quot;&amp;quot;+ &amp;quot;A port range to reserve for services with NodePort visibility. &amp;quot;+ &amp;quot;Example: &#39;30000-32767&#39;. Inclusive at both ends of the range.&amp;quot;)  其实上面注释已经说了是闭区间了，但是我还是要看看
搜索 ServiceNodePortRange 在
gopath/src/k8s.io/kubernetes/cmd/kube-apiserver/app/options/options.go  中存在
ServiceNodePortRange: kubeoptions.DefaultServiceNodePortRange,  跟踪 DefaultServiceNodePortRange 找到
var DefaultServiceNodePortRange = utilnet.PortRange{Base: 30000, Size: 2768}  跟踪 PortRange 找到
type PortRange struct { Base int Size int } // Contains tests whether a given port falls within the PortRange.</description>
    </item>
    
    <item>
      <title>k8s 源码分析: ReplicaSet 控制器</title>
      <link>http://www.recall704.com/cloud/k8s-replicaset/</link>
      <pubDate>Thu, 26 Apr 2018 15:32:03 +0800</pubDate>
      
      <guid>http://www.recall704.com/cloud/k8s-replicaset/</guid>
      <description>0x01 RS 控制器 func (rsc *ReplicaSetController) manageReplicas(filteredPods []*v1.Pod, rs *extensions.ReplicaSet) error { // 当前 pod 数量，期望 pod 数量 diff := len(filteredPods) - int(*(rs.Spec.Replicas)) rsKey, err := controller.KeyFunc(rs) if err != nil { utilruntime.HandleError(fmt.Errorf(&amp;quot;Couldn&#39;t get key for %v %#v: %v&amp;quot;, rsc.Kind, rs, err)) return nil } if diff &amp;lt; 0 { // 当前 pod 数量 &amp;lt; 期望 pod 数量 // 需要增加 pod diff *= -1 if diff &amp;gt; rsc.burstReplicas { // 如果一次要增加的 pod 过多，则对数量进行限制，分多次完成完成 diff = rsc.</description>
    </item>
    
    <item>
      <title>创建 client （三）： HTTP Basic Auth 访问 k8s API Server</title>
      <link>http://www.recall704.com/cloud/k8s-authentication-with-basic/</link>
      <pubDate>Sun, 15 Apr 2018 22:39:03 +0800</pubDate>
      
      <guid>http://www.recall704.com/cloud/k8s-authentication-with-basic/</guid>
      <description>0x01 结构体 AuthInfo 如果你留意过上一篇文章，你会发现一个结构体
AuthInfo: clientcmdapi.AuthInfo{ ClientCertificate: &amp;quot;client.crt&amp;quot;, ClientKey: &amp;quot;client.key&amp;quot;, },  其对应代码在 client-go 下的 k8s.io/client-go/tools/clientcmd/api 中
type AuthInfo struct { // LocationOfOrigin indicates where this object came from. It is used for round tripping config post-merge, but never serialized. LocationOfOrigin string // ClientCertificate is the path to a client cert file for TLS. // +optional ClientCertificate string `json:&amp;quot;client-certificate,omitempty&amp;quot;` // ClientCertificateData contains PEM-encoded data from a client cert file for TLS.</description>
    </item>
    
    <item>
      <title>创建 client （二）： 通过配置文件方式访问 k8s API</title>
      <link>http://www.recall704.com/cloud/k8s-authentication-with-ca/</link>
      <pubDate>Sun, 15 Apr 2018 22:12:03 +0800</pubDate>
      
      <guid>http://www.recall704.com/cloud/k8s-authentication-with-ca/</guid>
      <description>0x01 概述 前面我讲过集群内通过 ServiceAccount 生成 token 的方式访问 k8s API Server， 那么如果我们要访问 k8s api 的应用不是跑在 k8s 之内，而是在 k8s 集群之外运行呢？
这个时候，我们还有两种认证方式
 ca 证书认证
 basic 认证  0x02 kubectl 是如何访问 k8s 的？ k8s 部署的时候，我们会给 kubectl 配置证书，context 等信息，生成一个 $HOME/.kube/config 这样的文件，文件内容如下 apiVersion: v1 clusters: - cluster: certificate-authority-data: ... server: https://192.168.1.200:6443 name: kube_cluster contexts: - context: cluster: kube_cluster namespace: kube-system user: kubectl name: kubectl@kube_cluster current-context: kubectl@kube_cluster kind: Config preferences: {} users: - name: kubectl user: as-user-extra: {} client-certificate-data: .</description>
    </item>
    
    <item>
      <title>k8s operator 简介</title>
      <link>http://www.recall704.com/cloud/k8s-operator/</link>
      <pubDate>Fri, 13 Apr 2018 10:03:03 +0800</pubDate>
      
      <guid>http://www.recall704.com/cloud/k8s-operator/</guid>
      <description>0x1 Operator 介绍 Operator 是 CoreOS 公司提出的一个概念，用来创建、配置、管理复杂应用，由两部分构成：
 1. Resource
 1.1 自定义资源
 1.2 为用户提供一种简单的方式描述对服务的期望
  2. Controller
 2.1 创建 Resource
 2.2 监听 Resource 的变更，用来实现用户对服务的期望
   0x2 Operator 工作流程  1. 注册自定义资源 CustomResource
 2. 监听(watch)自定义资源
 3. 用户对自定义进行 CREATE/UPDATE/DELETE 操作
 4. 当监听到资源变化之后，调用对应的 handler 进行处理
  如果你了解 k8s 的内部原理，你会发现其实 k8s 本身就是这样做的，
比如 deployment ，只是 deployment 资源是内置资源，
不用向 APIServer 进行注册了而已，deployment controller 会对资源进行监控和处理。</description>
    </item>
    
    <item>
      <title>创建 client （一）： token 方式访问 k8s API</title>
      <link>http://www.recall704.com/cloud/k8s-authentication-with-token/</link>
      <pubDate>Wed, 11 Apr 2018 14:22:03 +0800</pubDate>
      
      <guid>http://www.recall704.com/cloud/k8s-authentication-with-token/</guid>
      <description>0x1 概述 k8s 提供了三种认证方式访问 API Server，分别是：
1. ca 认证 k8s 的各个组件，比如 kubelet, kube-proxy 和 API Server 进行通信的时候，一般都是使用证书的形式，配置在 kubeconfig 文件中。
2. token 认证 rook （https://github.com/rook/rook） 、dashboard （https://github.com/kubernetes/dashboard） 这些应用，跑在 k8s 集群内，而且需要访问 k8s 的资源， 运行的时候需要指定 ServiceAccount ，ServiceAccount 又配置了指定的角色和权限。
3. http basic 认证 使用用户名和密码访问 k8s，稍后会有新的文章详细介绍。
0x2 token 认证详解 像 rook、dashboard 这样的应用，它是怎样通过 ServiceAccount 和 k8s API Server 进行通信的呢？
和 k8s API Server 通信需要创建 client， 而创建 client 需要对应的 配置信息（包含了 API Server 的地址和端口，以及认证信息）
我们在 rook 的代码中，可以看到下面这样类似的代码</description>
    </item>
    
    <item>
      <title>k8s 中 node 的 roles</title>
      <link>http://www.recall704.com/cloud/k8s-node-roles/</link>
      <pubDate>Wed, 14 Mar 2018 14:22:53 +0800</pubDate>
      
      <guid>http://www.recall704.com/cloud/k8s-node-roles/</guid>
      <description>部署 k8s 之后，get nodes 可能会发现 node 上有个 ROLES
但是执行命令：
kubectl get nodes 192.168.88.201 -o yaml  在里面却没有发现有 ROLES 这样的字段？
那我们如果要给 node 添加对应的 ROLE 到底要怎么做呢？
通过代码，我们可以知道，k8s 把 label node-role.kubernetes.io/&amp;lt;role&amp;gt;=&amp;quot;&amp;quot; 和 kubernetes.io/role=&amp;quot;&amp;lt;role&amp;gt;&amp;quot; 来定义角色。
所以 master 可以这样
kubectl label node 192.168.88.201 node-role.kubernetes.io/master=true  或者
kubectl label node 192.168.88.201 kubernetes.io/role=master  </description>
    </item>
    
    <item>
      <title>k8s 中的污染 Taint 与容忍 Toleration</title>
      <link>http://www.recall704.com/cloud/taint-and-toleration/</link>
      <pubDate>Sun, 11 Mar 2018 22:36:53 +0800</pubDate>
      
      <guid>http://www.recall704.com/cloud/taint-and-toleration/</guid>
      <description>之前的内部分享中我提到过，pod 最终会被调度到某个具体的 node 上运行。那么有没有某种方式，限制 pod，不让它在某些 node 上运行呢？ 或者说，只允许某些 pod 在某些 node 上运行呢？  taint 和 toleration 了解一下。
 一、 taint （污染） 首先一点，taint 是针对 node 而言的，给 node 增加 taint 的命令如下
kubectl taint nodes node1 aaa=bbb:NoSchedule  其中：
1. aaa 代表 taint 的 key
2. bbb 代表 taint 的 value
3. NoSchedule 代码这个 taint 对应的影响，它可以有三个值
   值 含义     NoSchedule 不允许新的 pod 调度到该 node 上，除非 toleration 条件满足；不影响已经运行在该 node 上的 pod   PreferNoSchedule 尽可能不要调度到该 node 上，除非 toleration 条件满足；不影响已经运行在该 node 上的 pod   NoExecute 驱逐该节点上的 pod 到其它节点，除了 toleration 条件满足的 pod；影响已经运行在该 node 上的所有 pod    我们了解到一点，那就是增加了 taint 的 node，pod 就不会调度到它上面运行了。</description>
    </item>
    
  </channel>
</rss>